{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Objective:\n",
        "The goal of this exercise is to assess your ability to implement, train, and optimize neural\n",
        "network architectures, particularly focusing on transformers and multi-task learning extensions.\n",
        "Please explain any and all choices made in the course of this assessment.\n",
        "## Task 1: Sentence Transformer Implementation\n",
        "Implement a sentence transformer model using any deep learning framework of your choice.\n",
        "This model should be able to encode input sentences into fixed-length embeddings. Test your\n",
        "implementation with a few sample sentences and showcase the obtained embeddings.\n",
        "Describe any choices you had to make regarding the model architecture outside of the\n",
        "transformer backbone"
      ],
      "metadata": {
        "id": "wcKyL1Lp-gwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe8XD_bm8MBJ",
        "outputId": "03816f89-f833-41fe-f00a-ebb90a06ef60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Embeddings:\n",
            "tensor([[ 0.0342, -0.2481, -0.1054,  ..., -0.0869, -0.1297,  0.2620],\n",
            "        [ 0.0206, -0.2149, -0.0401,  ...,  0.1628,  0.1432,  0.0116],\n",
            "        [ 0.0234, -0.1262, -0.1055,  ..., -0.1875, -0.5122, -0.1344]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class SentenceTransformerModel(nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased'):\n",
        "        super(SentenceTransformerModel, self).__init__()\n",
        "        # Load the pre-trained transformer backbone\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the transformer outputs\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = outputs.last_hidden_state  # shape: (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Masked Mean Pooling: Only average over non-padding tokens\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        sentence_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        return sentence_embeddings\n",
        "\n",
        "    def encode(self, sentences, max_length=128, device='cpu'):\n",
        "        self.to(device)\n",
        "        # Tokenize the input sentences\n",
        "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True,\n",
        "                                       max_length=max_length, return_tensors='pt')\n",
        "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.forward(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "        return embeddings.cpu()\n",
        "\n",
        "# Testing the model with sample sentences\n",
        "if __name__ == '__main__':\n",
        "    model = SentenceTransformerModel('distilbert-base-uncased')\n",
        "    sentences = [\n",
        "        \"This is a test sentence.\",\n",
        "        \"Here is another one.\",\n",
        "        \"Sentence transformers are useful for NLP tasks.\"\n",
        "    ]\n",
        "    embeddings = model.encode(sentences)\n",
        "    print(\"Sentence Embeddings:\")\n",
        "    print(embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Multi-Task Learning Expansion\n",
        "Expand the sentence transformer to handle a multi-task learning setting.\n",
        "1. Task A: Sentence Classification – Classify sentences into predefined classes (you can make these up).\n",
        "\n",
        "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition, Sentiment Analysis, etc.] (you can make the labels up)\n",
        "Describe the changes made to the architecture to support multi-task learning"
      ],
      "metadata": {
        "id": "uv3WD--oBmw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Task Sentence Transformer Model\n",
        "\n",
        "    This model uses a shared pre-trained transformer (e.g., DistilBERT) as a backbone to encode sentences into\n",
        "    fixed-length embeddings. It includes two task-specific heads:\n",
        "      - Task A: Sentence Classification head for predicting predefined classes.\n",
        "      - Task B: Sentiment Analysis head for predicting sentiment classes.\n",
        "\n",
        "    The shared sentence embeddings are computed using masked mean pooling, ensuring that only valid (non-padding)\n",
        "    tokens contribute to the final representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='distilbert-base-uncased',\n",
        "                 num_classification_classes=3, num_sentiment_classes=3):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "        # Load the pre-trained transformer backbone and its tokenizer.\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # Hidden size comes from the transformer model's configuration.\n",
        "        self.hidden_size = self.transformer.config.hidden_size\n",
        "\n",
        "        # Define the Sentence Classification Head (Task A).\n",
        "        self.classification_head = nn.Linear(self.hidden_size, num_classification_classes)\n",
        "\n",
        "        # Define the Sentiment Analysis Head (Task B).\n",
        "        self.sentiment_head = nn.Linear(self.hidden_size, num_sentiment_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, sequence_length).\n",
        "            attention_mask (torch.Tensor): Attention mask to differentiate valid tokens from padding.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - 'sentence_embedding': The pooled sentence embeddings.\n",
        "                - 'classification_logits': Logits from the classification head.\n",
        "                - 'sentiment_logits': Logits from the sentiment analysis head.\n",
        "        \"\"\"\n",
        "        # Obtain hidden states from the transformer.\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = outputs.last_hidden_state  # (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Masked mean pooling: Average only over non-padding tokens.\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        sentence_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        # Compute outputs for both tasks.\n",
        "        classification_logits = self.classification_head(sentence_embeddings)\n",
        "        sentiment_logits = self.sentiment_head(sentence_embeddings)\n",
        "\n",
        "        return {\n",
        "            'sentence_embedding': sentence_embeddings,\n",
        "            'classification_logits': classification_logits,\n",
        "            'sentiment_logits': sentiment_logits\n",
        "        }\n",
        "\n",
        "    def encode(self, sentences, max_length=128, device='cpu'):\n",
        "        \"\"\"\n",
        "        Encodes a list of sentences to obtain the shared sentence embeddings along with\n",
        "        task-specific outputs.\n",
        "\n",
        "        Args:\n",
        "            sentences (list[str]): List of input sentences.\n",
        "            max_length (int): Maximum token length for each sentence.\n",
        "            device (str): Device to perform computations on ('cpu' or 'cuda').\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with sentence embeddings and task-specific logits.\n",
        "        \"\"\"\n",
        "        # Ensure model is on the desired device.\n",
        "        self.to(device)\n",
        "        # Tokenize the input sentences.\n",
        "        encoded_input = self.tokenizer(\n",
        "            sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt'\n",
        "        )\n",
        "        # Move tokenized inputs to the device.\n",
        "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
        "        # Perform the forward pass without gradient tracking.\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "        return outputs\n",
        "\n",
        "def test_multitask_model():\n",
        "    \"\"\"\n",
        "    Tests the MultiTaskSentenceTransformer by encoding a list of sample sentences.\n",
        "    It prints out the sentence embeddings and task-specific logits for easy inspection.\n",
        "    \"\"\"\n",
        "    # Create an instance of the multi-task model.\n",
        "    model = MultiTaskSentenceTransformer('distilbert-base-uncased')\n",
        "\n",
        "    # Define sample sentences to be encoded.\n",
        "    sentences = [\n",
        "        \"This is a test sentence.\",\n",
        "        \"I love machine learning and AI!\",\n",
        "        \"Can you tell me the weather forecast?\"\n",
        "    ]\n",
        "\n",
        "    # Obtain outputs from the model.\n",
        "    outputs = model.encode(sentences)\n",
        "\n",
        "    # Display the outputs with descriptive labels.\n",
        "    print(\"=== Sentence Embeddings ===\")\n",
        "    print(outputs['sentence_embedding'])\n",
        "    print(\"\\n=== Sentence Classification Logits ===\")\n",
        "    print(outputs['classification_logits'])\n",
        "    print(\"\\n=== Sentiment Analysis Logits ===\")\n",
        "    print(outputs['sentiment_logits'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Execute the test function when the script is run directly.\n",
        "    test_multitask_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-CxqlOB8ZdG",
        "outputId": "e2f7cab0-86b0-4ca1-88f1-266bb4045fe6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sentence Embeddings ===\n",
            "tensor([[ 0.0342, -0.2481, -0.1054,  ..., -0.0869, -0.1297,  0.2620],\n",
            "        [ 0.1627,  0.2393, -0.0936,  ..., -0.0640,  0.1666,  0.0617],\n",
            "        [ 0.1711, -0.2164,  0.1976,  ..., -0.0619,  0.2769,  0.1126]])\n",
            "\n",
            "=== Sentence Classification Logits ===\n",
            "tensor([[-0.1796, -0.1621,  0.2072],\n",
            "        [-0.1913, -0.1343, -0.0164],\n",
            "        [ 0.0315, -0.1127, -0.0348]])\n",
            "\n",
            "=== Sentiment Analysis Logits ===\n",
            "tensor([[-0.0830,  0.0332,  0.3141],\n",
            "        [-0.2252, -0.1993,  0.1684],\n",
            "        [-0.0978,  0.1167,  0.1811]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1: Freezing the Entire Network\n",
        "What It Means:\n",
        "When you freeze every part of the model—including the transformer backbone and the task-specific heads—you prevent any parameters from updating during training. The model essentially works as a fixed feature extractor.\n",
        "\n",
        "## Pros:\n",
        "\n",
        "Speed and Simplicity: Without any weight updates, the training process is extremely fast and computationally light.\n",
        "Low Overfitting Risk: In scenarios where you have a very small dataset, this approach helps prevent overfitting since the model’s parameters remain unchanged.\n",
        "## When to Use:\n",
        "If you’re working in a domain very similar to what the model was originally trained on, or if you have very little new data, freezing the entire network can be a practical option. However, keep in mind that this approach doesn’t allow the model to learn any domain-specific nuances.\n",
        "\n",
        "#Scenario 2: Freezing Only the Transformer Backbone\n",
        "What It Means:\n",
        "In this case, the backbone—the pre-trained transformer—is kept unchanged, while the task-specific layers (the classification and sentiment heads) are fine-tuned on your new data.\n",
        "\n",
        "##Pros:\n",
        "\n",
        "Balanced Adaptation: The rich language features captured by the transformer are preserved, while the heads learn to map these features to your specific labels.\n",
        "Efficient Learning: Since only a subset of the parameters is updated, training is faster and less prone to overfitting, which is particularly useful if your new dataset is limited in size.\n",
        "\n",
        "##When to Use:\n",
        "This method is ideal when the pre-trained model already provides strong general features that work well for your tasks, but you need some adaptation to capture the specifics of your new domain or task. It’s a common choice in transfer learning scenarios.\n",
        "\n",
        "#Scenario 3: Freezing Only One Task-Specific Head\n",
        "What It Means:\n",
        "Here, you choose to freeze one of the task heads (for instance, the sentiment analysis head) while allowing the transformer backbone and the other head (e.g., sentence classification) to be updated.\n",
        "\n",
        "##Pros:\n",
        "\n",
        "Targeted Stability: If one of the tasks already has a well-performing head (perhaps because you’ve already tuned it or it has plenty of data), freezing it can protect its performance.\n",
        "Focused Improvement: Meanwhile, the other parts of the model can continue to learn and adapt, which is beneficial when the two tasks have different levels of maturity or data quality.\n",
        "##When to Use:\n",
        "\n",
        "This approach makes sense if you have one task that is already reliable and you want to maintain its performance, while still improving on the other task. It allows for selective fine-tuning without disrupting the strengths of the better-performing head.\n",
        "\n",
        "In a scenario where you have a limited, domain-specific dataset, transfer learning can significantly boost performance by leveraging robust features from a model pre-trained on massive, diverse data. Here’s how I would approach the transfer learning process:\n",
        "\n",
        "## 1. Choosing a Pre-Trained Model\n",
        "I’d select a well-established transformer model such as DistilBERT, BERT, or RoBERTa. For instance, DistilBERT is a great candidate because it’s a lighter, faster version of BERT that still retains much of its power. These models have been trained on extensive, diverse datasets, enabling them to capture rich language representations that are broadly useful across many tasks.\n",
        "\n",
        "## 2. Freezing and Unfreezing Layers\n",
        "Initial Phase:\n",
        "\n",
        "Freeze the Transformer Backbone:\n",
        "Start by freezing most, if not all, of the transformer layers. In this phase, only the task-specific heads (e.g., classification or sentiment analysis layers) are updated. This approach leverages the pre-trained knowledge without risking overfitting on a small new dataset.\n",
        "Gradual Unfreezing:\n",
        "\n",
        "Unfreeze Later Layers Gradually:\n",
        "Once the task-specific heads have adapted to your dataset, begin unfreezing the later layers of the transformer. These layers tend to capture more specialized, task-relevant features. The early layers, which extract very general features (such as syntax), often remain frozen longer because their representations are broadly transferable.\n",
        "This method, sometimes called discriminative fine-tuning, allows the model to gradually adjust to the specifics of your new domain while preserving the valuable general language patterns learned during pre-training.\n",
        "\n",
        "## 3. Rationale Behind These Choices\n",
        "Preserving Robust Representations:\n",
        "The pre-trained model has already learned strong language features. Freezing the backbone initially helps retain this knowledge, ensuring that the model doesn’t \"unlearn\" these valuable representations during early fine-tuning.\n",
        "\n",
        "Preventing Overfitting:\n",
        "With a small or domain-specific dataset, training the entire network might lead to overfitting. By only training the task-specific heads at first, you reduce the risk of overfitting, as fewer parameters are updated.\n",
        "\n",
        "Efficient Use of Resources:\n",
        "Updating only the task-specific layers means fewer parameters are being tuned initially, which speeds up training and reduces computational demands.\n",
        "\n",
        "Controlled Adaptation:\n",
        "Gradually unfreezing the later layers allows the model to slowly adapt to the nuances of your new data. This controlled approach helps balance the need to incorporate domain-specific features with the risk of losing the general language understanding provided by the pre-trained layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "UWGT6c9lC9QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Training Loop Implementation (BONUS)\n",
        "If not already done, code the training loop for the Multi-Task Learning Expansion in Task 2.\n",
        "Explain any assumptions or decisions made paying special attention to how training within a\n",
        "MTL framework operates. Please note you need not actually train the model.\n",
        "Things to focus on:\n",
        "● Handling of hypothetical data\n",
        "● Forward pass\n",
        "● Metrics"
      ],
      "metadata": {
        "id": "Iy_b2_AfF2OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Multi-Task Model Definition\n",
        "# -----------------------------------------------------------------------------\n",
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-task Sentence Transformer Model that uses a shared transformer backbone\n",
        "    and two task-specific heads: one for sentence classification and one for sentiment analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='distilbert-base-uncased',\n",
        "                 num_classification_classes=3, num_sentiment_classes=3):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "        # Load the pre-trained transformer backbone and its tokenizer.\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.hidden_size = self.transformer.config.hidden_size\n",
        "\n",
        "        # Task A: Sentence Classification Head.\n",
        "        self.classification_head = nn.Linear(self.hidden_size, num_classification_classes)\n",
        "        # Task B: Sentiment Analysis Head.\n",
        "        self.sentiment_head = nn.Linear(self.hidden_size, num_sentiment_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass that returns shared embeddings along with task-specific logits.\n",
        "        \"\"\"\n",
        "        # Obtain token-level outputs from the transformer.\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = outputs.last_hidden_state  # (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Masked Mean Pooling: Only average non-padding tokens.\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        sentence_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        # Task-specific logits.\n",
        "        classification_logits = self.classification_head(sentence_embeddings)\n",
        "        sentiment_logits = self.sentiment_head(sentence_embeddings)\n",
        "\n",
        "        return {\n",
        "            'sentence_embedding': sentence_embeddings,\n",
        "            'classification_logits': classification_logits,\n",
        "            'sentiment_logits': sentiment_logits\n",
        "        }\n",
        "\n",
        "    def encode(self, sentences, max_length=128, device='cpu'):\n",
        "        \"\"\"\n",
        "        Encodes a list of sentences by tokenizing them and performing a forward pass.\n",
        "        Note: This method is useful for inference where gradients are not needed.\n",
        "        \"\"\"\n",
        "        self.to(device)\n",
        "        encoded_input = self.tokenizer(\n",
        "            sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt'\n",
        "        )\n",
        "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "        return outputs\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Dummy Dataset for Multi-Task Learning\n",
        "# -----------------------------------------------------------------------------\n",
        "class DummyMultiTaskDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A simple dummy dataset where each sample consists of a sentence,\n",
        "    a classification label, and a sentiment label.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Hypothetical data with made-up labels.\n",
        "        self.data = [\n",
        "            (\"This is a test sentence.\", 0, 1),\n",
        "            (\"I love machine learning and AI!\", 1, 0),\n",
        "            (\"Can you tell me the weather forecast?\", 2, 2),\n",
        "            (\"What a wonderful day!\", 1, 0),\n",
        "            (\"I don't like this movie.\", 0, 2),\n",
        "            (\"How do I reset my password?\", 2, 1)\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, class_label, sentiment_label = self.data[idx]\n",
        "        return {\n",
        "            \"sentence\": sentence,\n",
        "            \"classification_label\": torch.tensor(class_label, dtype=torch.long),\n",
        "            \"sentiment_label\": torch.tensor(sentiment_label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_model(model, dataloader, optimizer, criterion, device, num_epochs=50):\n",
        "    \"\"\"\n",
        "    Train the multi-task model over a number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The multi-task model.\n",
        "        dataloader (DataLoader): DataLoader for the dataset.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss).\n",
        "        device (str): Device to run training on (e.g., 'cpu' or 'cuda').\n",
        "        num_epochs (int): Number of epochs to train.\n",
        "    \"\"\"\n",
        "    print(\"Starting Training Loop for Multi-Task Learning Model...\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode.\n",
        "        epoch_loss = 0.0\n",
        "        classification_correct = 0\n",
        "        sentiment_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            sentences = batch['sentence']\n",
        "            class_labels = batch['classification_label'].to(device)\n",
        "            sentiment_labels = batch['sentiment_label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients for the batch.\n",
        "\n",
        "            # Tokenize the sentences and move them to the correct device.\n",
        "            encoded_input = model.tokenizer(\n",
        "                sentences, padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
        "            )\n",
        "            encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
        "\n",
        "            # Forward pass: compute model outputs for the batch.\n",
        "            outputs = model.forward(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "            classification_logits = outputs['classification_logits']\n",
        "            sentiment_logits = outputs['sentiment_logits']\n",
        "\n",
        "            # Compute loss for each task.\n",
        "            loss_classification = criterion(classification_logits, class_labels)\n",
        "            loss_sentiment = criterion(sentiment_logits, sentiment_labels)\n",
        "\n",
        "            # Total loss: sum of individual losses (equal weighting assumed).\n",
        "            total_loss = loss_classification + loss_sentiment\n",
        "\n",
        "            # Backpropagation.\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and correct predictions.\n",
        "            batch_size = len(sentences)\n",
        "            epoch_loss += total_loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "            _, predicted_class = torch.max(classification_logits, dim=1)\n",
        "            _, predicted_sentiment = torch.max(sentiment_logits, dim=1)\n",
        "            classification_correct += (predicted_class == class_labels).sum().item()\n",
        "            sentiment_correct += (predicted_sentiment == sentiment_labels).sum().item()\n",
        "\n",
        "        # Calculate average loss and accuracy for the epoch.\n",
        "        avg_loss = epoch_loss / total_samples\n",
        "        classification_accuracy = classification_correct / total_samples\n",
        "        sentiment_accuracy = sentiment_correct / total_samples\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Classification Accuracy: {classification_accuracy:.4f}\")\n",
        "        print(f\"Sentiment Accuracy: {sentiment_accuracy:.4f}\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Function to Run Training\n",
        "# -----------------------------------------------------------------------------\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Sets up the dataset, model, optimizer, and initiates training.\n",
        "    \"\"\"\n",
        "    # Determine the device (GPU if available, else CPU).\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Create the dummy dataset and DataLoader.\n",
        "    dataset = DummyMultiTaskDataset()\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Initialize the multi-task model.\n",
        "    model = MultiTaskSentenceTransformer('distilbert-base-uncased', num_classification_classes=3, num_sentiment_classes=3)\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up the optimizer and loss function.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Run the training loop.\n",
        "    train_model(model, dataloader, optimizer, criterion, device, num_epochs=50)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7fLVCK481i_",
        "outputId": "96ecfec6-56ca-4915-d50a-c6faa2d36196"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop for Multi-Task Learning Model...\n",
            "\n",
            "Epoch 1/50\n",
            "Average Loss: 2.2785\n",
            "Classification Accuracy: 0.1667\n",
            "Sentiment Accuracy: 0.1667\n",
            "--------------------------------------------------\n",
            "Epoch 2/50\n",
            "Average Loss: 2.0022\n",
            "Classification Accuracy: 0.6667\n",
            "Sentiment Accuracy: 0.5000\n",
            "--------------------------------------------------\n",
            "Epoch 3/50\n",
            "Average Loss: 1.8470\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 0.8333\n",
            "--------------------------------------------------\n",
            "Epoch 4/50\n",
            "Average Loss: 1.6635\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 0.8333\n",
            "--------------------------------------------------\n",
            "Epoch 5/50\n",
            "Average Loss: 1.4878\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 6/50\n",
            "Average Loss: 1.2600\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 7/50\n",
            "Average Loss: 1.0454\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 8/50\n",
            "Average Loss: 0.8103\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 9/50\n",
            "Average Loss: 0.6237\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 10/50\n",
            "Average Loss: 0.4737\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 11/50\n",
            "Average Loss: 0.3773\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 12/50\n",
            "Average Loss: 0.2667\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 13/50\n",
            "Average Loss: 0.2118\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 14/50\n",
            "Average Loss: 0.1631\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 15/50\n",
            "Average Loss: 0.1355\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 16/50\n",
            "Average Loss: 0.1157\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 17/50\n",
            "Average Loss: 0.0940\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 18/50\n",
            "Average Loss: 0.0819\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 19/50\n",
            "Average Loss: 0.0707\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 20/50\n",
            "Average Loss: 0.0648\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 21/50\n",
            "Average Loss: 0.0576\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 22/50\n",
            "Average Loss: 0.0497\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 23/50\n",
            "Average Loss: 0.0479\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 24/50\n",
            "Average Loss: 0.0416\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 25/50\n",
            "Average Loss: 0.0410\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 26/50\n",
            "Average Loss: 0.0377\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 27/50\n",
            "Average Loss: 0.0354\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 28/50\n",
            "Average Loss: 0.0335\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 29/50\n",
            "Average Loss: 0.0304\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 30/50\n",
            "Average Loss: 0.0292\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 31/50\n",
            "Average Loss: 0.0285\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 32/50\n",
            "Average Loss: 0.0272\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 33/50\n",
            "Average Loss: 0.0252\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 34/50\n",
            "Average Loss: 0.0247\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 35/50\n",
            "Average Loss: 0.0241\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 36/50\n",
            "Average Loss: 0.0225\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 37/50\n",
            "Average Loss: 0.0221\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 38/50\n",
            "Average Loss: 0.0213\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 39/50\n",
            "Average Loss: 0.0202\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 40/50\n",
            "Average Loss: 0.0207\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 41/50\n",
            "Average Loss: 0.0187\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 42/50\n",
            "Average Loss: 0.0188\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 43/50\n",
            "Average Loss: 0.0184\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 44/50\n",
            "Average Loss: 0.0173\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 45/50\n",
            "Average Loss: 0.0169\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 46/50\n",
            "Average Loss: 0.0166\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 47/50\n",
            "Average Loss: 0.0163\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 48/50\n",
            "Average Loss: 0.0156\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 49/50\n",
            "Average Loss: 0.0148\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n",
            "Epoch 50/50\n",
            "Average Loss: 0.0148\n",
            "Classification Accuracy: 1.0000\n",
            "Sentiment Accuracy: 1.0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Brief Write-Up: Key Decisions and Insights\n",
        "\n",
        "## Task 3\n",
        "For Task 3, my training considerations revolved around striking a balance between leveraging pre-trained knowledge and adapting the model to new, task-specific data. I evaluated three freezing strategies:\n",
        "\n",
        "### Freezing the Entire Network:\n",
        "This approach treats the model as a fixed feature extractor, which is fast and minimizes overfitting with very little data but limits domain adaptation.\n",
        "\n",
        "### Freezing Only the Transformer Backbone:\n",
        "Here, the robust, pre-trained layers remain untouched while the task-specific heads are fine-tuned. This method preserves general language features and is efficient when data is limited, yet it still allows customization to your new tasks.\n",
        "\n",
        "### Freezing Only One Task-Specific Head:\n",
        "Selectively freezing one head protects the performance of a well-established task, allowing the rest of the network to adapt to improve the other task. This is useful when tasks vary in maturity or label quality.\n",
        "\n",
        "For transfer learning, I would begin with a model like DistilBERT, which offers a good balance of speed and accuracy, freeze the backbone initially to preserve its learned representations, and then gradually unfreeze the later layers for fine-tuning. This strategy minimizes overfitting while ensuring that the model adapts smoothly to the new domain.\n",
        "\n",
        "## Task 4\n",
        "In Task 4, the training loop was designed to handle hypothetical data by creating a dummy dataset that simulates the multi-task scenario. Key decisions included:\n",
        "\n",
        "### Handling Data:\n",
        "Using a synthetic dataset to demonstrate how sentences and their associated labels can be batched and fed to the model.\n",
        "\n",
        "### Forward Pass and Loss Computation:\n",
        "The model computes sentence embeddings via masked mean pooling and produces logits for each task. Losses for classification and sentiment analysis are computed using cross-entropy loss and summed to provide a single training signal.\n",
        "\n",
        "### Metrics:\n",
        "Accuracy for each task is tracked by comparing predictions with ground truth, which helps monitor training progress.\n",
        "\n",
        "This overall approach ensures that the model benefits from strong pre-trained features while being effectively fine-tuned for multi-task learning, balancing adaptation with the risk of overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uV1MYuU0Hes6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OCMPXZJyFzJI"
      }
    }
  ]
}